%!TEX root = ../thesis.tex

\chapter{Implications of the thesis} \label{chap:implications}

%In the previous two chapters, I related findings of the case study to key claims in the literature, and discussed the affordances of the developed tools and methods for future research. In this chapter, I provide a sketch of the implications of the work for the research areas 

In the previous chapter, I addressed the first two research questions, concerning \glslink{lexicogrammar}{lexicogrammatical} and semantic change in the \glslink{Forum}{Bipolar Forum}. In this chapter, I address the third and fourth research questions. First, I outline the implications of the case study by research area, reflecting on what the case study means for \gls{CL} (and \gls{CADS} in particular), for \gls{SFL} theory, and to the body of literature centred on healthcare communication (HC), both in online and offline contexts. With these implications in mind, I then address the final research question, which concerns the development of novel tools and methods for \gls{CL}.

\section{Corpus linguistics}

The methods employed in the case study fall, for the most part, within the domain of \gls{CL}. In other ways, however, the methods departed from what is typical of \gls{CL}: the use of symbolic subcorpus structures and traversal of parser output, for example, are tasks more common in computational linguistics. This departure from `mainstream' \gls{CL} methods was necessitated by the specific nature of the research questions and dataset: interest was not in determining how the linguistic features of the \gls{Forum} differ from other communities, or from language use generally. Rather, the case study sought to elucidate the internal structure of the community, and the effect of this structure on users' linguistic choices. As explained in Chapter \ref{chap:researchdesign}, further important key aims were to develop tools and strategies for increasing the transparency and reproducibility of \gls{CL} results, and the ability to easily apply developed methods to new datasets for future work, either for the purpose of extending the analysis of the \glslink{Forum}{Bipolar Forum}, or applying the methods to entirely new domains.

%Furthermore, a goal was to model the \emph{entire} community, rather than to sample it and perform manual analysis of samples. 

In contrast to the approach taken here, much contemporary \gls{CL} is still deeply influenced by the methodological parameters of the Brown Corpus investigations of the early 1960s. In that era, \glspl{corpus} were expensive to construct, annotate, store and transmit; methods for automatic value\hyp{}adding and querying were essentially unavailable. Today, \glspl{corpus} are still typically understood as being static, often monolithic collections of texts, with only the best\hyp{}known\slash funded \glspl{corpus} receiving periodic updates (e.g. the BNC, COCA). Many of these \glspl{corpus} are accessible to the researcher only via specific query interfaces, constraining the kinds of things that can be searched for, and thus ultimately, the kinds of research questions that can be answered. As many of these interfaces are graphical (including graphical web\hyp{}based tools), rather than programmatic, iterative and recursive searching of the \gls{corpus} is generally not possible. Again, this constrains what is methodologically possible.

Over the past decades, technological advances (from e.g. computer science, computational linguistics, and \gls{NLP}) have changed both what is feasible and what is possible for \gls{CL}. A major contribution of the case study is in bringing some of these technological advances into the purview of \gls{CL} research practice---this is a step forward not just for \gls{CL}, but for the \gls{NLP} community, who can see wider engagement with core tasks in their research area \cite{de2008stanford}. In the sections below, I outline key tools and tasks that have received little attention within \gls{CL}, and then describe their utility in the context of the case study and developed tools.


\subsection{Corpus structure and metadata}

A major limitation of existing \gls{CL} tools (especially of the graphical kind) has been the ability to work with the inherent structure of collections of texts used in \glspl{corpus}. Commonly, tools produce two dimensional results: a list of phenomena (e.g. words matching a query) and some kind of score for each (frequency, keyness, collocate strength, etc.). If a researcher is interested in discrete subcorpora, he\slash she may have to load each into the tool, run the same query, export results, and collate them in a separate tool. This is time-consuming, and increases the likelihood of making mistakes.

Tools that do have some awareness of subcorpora generally do this by treating each file or folder as a subcorpus. Therefore, a corpus must be duplicated and restructured in order to investigate different kinds of structures (for example, change over time and change over membership length). An issue within this approach, however, is that texts within a corpus may belong to multiple categories. In the case of the \glslink{Forum}{Bipolar Forum}, for example, each \gls{post} has a poster, a post count, a timestamp, and exists within a \gls{thread}. These are metadata features that can be easily encoded within a \gls{corpus}, and to which a tool can be sensitive during interrogation. \texttt{corpkit} solves this issue by transferring \gls{XML}\hyp{}formatted metadata from plain text into the parsed representation. These features can act as filters, where texts can be included\slash excluded based on some combination of metadata values, or can dynamically be treated by the tool as the subcorpora that comprise the corpus. Using the syntax of \texttt{corpkit}'s interpreter (Figure \ref{fig:interpreter-subcorpora}), it is trivial to investigate different dimensions within the same dataset. The first outputs results for the ten stages of membership; the second outputs annual subcorpora.

\begin{figure}[htb]
\begin{minted}[linenos,frame=single,xleftmargin=1cm,breaklines=true]{bash}
> search corpus for functions matching roles.process \
...     with subcorpora as postgroup
> search corpus for functions matching roles.process \
...     with subcorpora as year
\end{minted}
\caption[Switching between symbolic corpus structures]{Using the \texttt{corpkit} interpreter to switch between symbolic corpus structures}
\label{fig:interpreter-subcorpora}
\end{figure}

\subsection{Natural language processing tool use}

Tools for common computational linguistic tasks (tokenisation, lemmatisation, \gls{POS} tagging, parsing, and coreference resolution for example) have become faster, more widely available, and considerably more accurate in recent years, thanks to advances in statistical machine learning\slash \gls{NLP} \cite{manning1999foundations}. These tools make it possible to search for more complex or abstract features of \glspl{corpus} than can be extracted from plain text. Of these tasks, only tokenisation has been fully embraced within \gls{CL}, for the simple reason that word frequency counting is not possible without it. Lemmatisation, when performed, is often simply based on wordlists of lemma forms and possible inflections (as is possible in \emph{AntConc}). Such a method is unreliable, as \gls{POS} information is needed to correctly return a base form. Without this information, a number of serious errors are likely to result (\emph{human being} $\rightarrow$ \emph{human be}). \gls{POS} tagging and parsing are even more uncommon. As mentioned in Section \ref{sect:annotation}, a key cause of this is a still\hyp{}lingering historical skepticism toward the imposition of theory on \gls{corpus} data. \Gls{corpus} data, under this view, should instead inform theories of language \cite[e.g.][]{sinclair_trust_2004}.\endnote{It also needs to be borne in mind that even the tokenisation process constitutes a theoretical imposition on \gls{corpus} texts.} This conviction ignores the fact that different kinds of research aims rely to a greater or lesser extent on \gls{corpus} annotation \cite{anthony_critical_2013}. Indeed, grammars can be (and have successfully been) derived from unannotated \gls{corpus} data \cite[e.g.][]{hunston_pattern_2000}. Discursively oriented tasks however, have different needs, centered on discovering how social actors are represented, or how interactants make interpersonal demands upon one another \cite{gee_introduction_2013}. With very large datasets, or with datasets in which the phenomenon of interest occurs thousands of times or more \cite{zinn_changing_2015}, researchers must find a way to effectively count and code the syntagmatic behaviour of the construction under investigation. This means choosing between the combination of parsing and traversal of parsed data structures, or leaving the data unparsed and then sampling from it, in order to create a manageably sized set of results for manual classification. The latter approach is oftentimes far from ideal, of course, as it involves both removal of instances of a phenomenon from consideration, and resource\hyp{}intensive and error\hyp{}prone manual work. A further advance of these kinds of annotation, discussed in more detail below, is that they can in fact eliminate theoretically problematic \gls{corpus} interrogation practices, such as the use of stopword lists and reference \glspl{corpus}.

In the case study, all interrogation of the data relied on the output of the \texttt{Stanford CoreNLP} suite, which performs sentence splitting, tokenisation, \gls{POS} tagging, lemmatisation, and constituency\slash dependency parsing.\endnote{Coreference resolution and named entity recognition annotations were also available as annotators, but not used.} These processes made possible the automated extraction of \glslink{lexicogrammar}{lexicogrammatical} features that realise the \glspl{discourse-semantic} of \gls{corpus} texts. Almost all search queries used in the generation of findings involved constituency and dependency parse traversal, in order to match, as closely as possible, concepts from the \gls{SFG} (including but not limited to Subject, Finite, Modal and Polarity within the \sctext{Mood} system and participant\slash Thing, process\slash Event, Agent, circumstance and Epithet\slash Classifier within \sctext{Transitivity}). These go far beyond what is possible using the mainstream practices of \gls{CL}, such as regular\hyp{}expression based searching of plain text, or of \gls{POS} tagged or chunked data.

One good example of the utility of such methods is the analysis of \emph{being\slash having bipolar}. To calculate the proportion of attributions that are made by \emph{be}, \emph{have} and \emph{other relational processes}, lemmatisation, \gls{POS} tags and parsing must all be used. The search query is complex, with five specifications at minimum. First, the query must match only verbs filling Event, rather than auxiliary roles. Second, the lemma form of the match must be in a list of possible relational processes (\emph{be}, \emph{have}, \emph{seem}, \emph{sound}, etc.). Third, it must have a dependent with a pronominal \gls{POS} tag. Fourth, this pronoun should not be \emph{it} (\emph{It was the bipolar} is a false positive). Fifth, there should be another dependent, matching the unjargonised or jargonised terms for \gls{bipolar}. Two additional specifications are then required, in order to determine how the result should be returned (ideally, in lemma form), and what metadata feature should be used to delimit subcorpora (in this case, the post group, so that we can analyse the Membership Stage Structure). This very complex query can not be carried out using previously available tools. It can be easily expressed in the language of \texttt{corpkit}'s interpreter, however (Figure \ref{fig:complex-query}).

\begin{figure}
\begin{minted}[linenos,frame=single,xleftmargin=1cm,breaklines=true]{bash}
> search corpus for function matching roles.event \
...    and lemma matching processes.relational \
...    and dependent-pos matching 'PRP' \
...    and not dependent-word matching 'it' \
...    and dependent-word matching 'bp|bipolar' \
...    showing lemma \
...    with subcorpora as postgroup
\end{minted}
\caption[Complex query formulation]{Complex query formulation using the \texttt{corpkit} interpreter}
\label{fig:complex-query}
\end{figure}

%todo: make relevant?
%\subsection{Data collection tools}

%Another important domain is web\hyp{}scraping tools, which can extract linguistic data and metadata from both broad crawling of the web and targetted crawling of individual domains. These tools provide a useful way to move beyond the conceptualisation of a \gls{corpus} as a static collection of documents\endnote{Crawler\hyp{}like systems are embedded into the \emph{WebBootCat} \gls{corpus} creation tool, though this still results in static \glspl{corpus}.}. For the case study, a simple crawling tool was created to harvest \gls{Forum} \glspl{thread} and accompanying metadata. It is not difficult to imagine an extension of this process, where the crawler is set to re\hyp{}run at some regular interval. In this way, the \gls{corpus} could become dynamic, in the vein of dynamic \glspl{corpus} built on RSS feeds \cite{fairon_glossanet_2008,minocha_feed_2013}. The process could be extended still, however: incoming data could not only be parsed and added to the existing structure, but interrogated, with new results being automatically merged with older results. This kind of methodology would allow \gls{CL} to apply in real\hyp{}time to communities or particular events. In turn, this increases the number of downstream applications of \gls{CL} methods, and expands the kinds of research questions that can be asked and answered, away from the historical and toward emerging linguistic patterns.

\subsubsection{Incorporating programming}

%todo: copy edit, tired
Another key technological affordance relevant to contemporary \gls{CL} is an increased access to high\hyp{}level programming languages, as well as comprehensive online documentation for learning and troubleshooting. Python, the main language used for the case study and tool development, is one of a number of sensible choices for linguistic work. It is well\hyp{}known for its readable, English\hyp{}like syntax, and the ease with which it can be learned \cite{radenski_python_2006}. Python is high\hyp{}level enough to facilitate working efficiently with linguistic data at word\hyp{}level and above, and as such, is the language in which a large number of linguistic tools (including \texttt{NLTK}, \texttt{pattern}, \texttt{spaCy} and \texttt{UAM Corpus Tool}) are written.

A key advantage of programmatic approaches to \gls{CL} more generally is that they allow iterative and recursive data exploration. Most obviously, the case study employs this method to iterate over multiple different symbolic \gls{corpus} structures, representing change over time, or stages of membership. More powerful still is to apply this process to uncovering specific linguistic configurations within the \gls{corpus}. Searching for salient processes, for example, revealed the emphasis new \gls{Forum} users place on the \emph{diagnose} Event. It was then possible to count the kinds of participants and circumstances involved in diagnosis\hyp{}as\hyp{}Event, and how these shift over the course of membership. The \gls{API} example in Figure \ref{fig:recursive-code} locates the most key processes in the first subcorpus, and then searches for the participants in each.

\begin{figure}[htb]
\begin{minted}[linenos,frame=single,xleftmargin=1cm,breaklines=true]{python}
# a place to store recursive results
part_in_proc = {}
# search corpus for Events
proc = corpus.interrogate({F: roles.event}, show=L)
# calculate keyness and sort
proc = proc.edit('k', SELF, sort_by='total')
# get top 5 results
key_in_new = process.results.iloc[0].sort_values()[:5]
# recursively search for participants in this Event
for keyword in list(key_in_new):
    parts = corpus.interrogate({GF: roles.process,
                                GL: keyword,
                                F: roles.participant},
                                show=L)
    # store result
    part_in_proc[keyword] = parts.results
\end{minted}
\caption[Recursive corpus investigation]{Recursive investigation via the \texttt{corpkit} API}
\label{fig:recursive-code}
\end{figure}
%

% The use of linear\hyp{}regression based result sorting made it possible to automatically determine which lexical and\slash or grammatical features were undergoing the most and least shift over the course of membership, and over the evolution of the forum itself. At the same time, the approach ensured that shifts in relative frequency were statistically significant.

In addition to linguistic tools, programmatic approaches to \gls{CL} also make it possible to interface with more general tools for result manipulation, statistical analysis and visualisation. Because these kinds of tasks are common within many sciences, as well as in industry, available tools are generally fast, stable, powerful and actively maintained. In terms of future \gls{corpus} tool design, \textcite{anthony_critical_2013} has highlighted the importance of modularity, and the ability to interact with these powerful third\hyp{}party libraries. In the analysis of the \emph{diagnose} process, for example, \texttt{SciPy} \cite{scipy2001} was used to perform linear regression analysis, in order to sort participants and circumstances into those becoming more and less frequent over time, uncovering differences in the ways in which diagnosis was construed by new and veteran \glslink{member}{users}. At the same time, the approach ensured that shifts in relative frequency were statistically significant. Dedicated \gls{corpus} query interfaces generally limit the extent to which a researcher can engage with these kinds of tools. Continuing the earlier mock investigation of \emph{being}\slash \emph{having} \gls{bipolar}, we can see how these kinds of tools are useful in turning the absolute frequency results into a figure that shows longitudinal change in Forum users' linguistic preferences. Figure \ref{fig:edit-calc-vis} provides an example of result merging and relative frequnecy calculation via \texttt{pandas} \cite{mckinney_pandas_2010}, sorting via \texttt{scipy} \cite{scipy2001} and visualisation via \texttt{Matplotlib} \cite{matplotlib_2007}:

\begin{figure}[htb]
\begin{minted}[linenos,frame=single,xleftmargin=1cm,breaklines=true]{bash}
# pandas: merge result columns
> edit result by merging entries not matching [be, have] as 'Other'
# pandas: make relative frequencies for each subcorpus
> calculate edited as percentage of self
# scipy: calculate trend lines and sort
> sort edited by increase
# matplotlib: visualise output
> plot edited as line chart \
...    with title 'Ascriptions of bipolar disorder' \
...    with x_label as 'Membership stage' \
...    and y_label as 'Pecentage of all ascriptions'
\end{minted}
\caption[Editing, sorting and visualising results]{Editing, sorting and visualising results with the \texttt{corpkit} interpreter}
\label{fig:edit-calc-vis}
\end{figure}
%
%Finally, programming allows researchers to leave tools and user interfaces behind, building unique workflows to solve particular problems. To date, much research in the field has been hampered by the inability of the researcher to code novel workflows and share those that perform well. Particularly undesirable are scenarios where data can only be accessed within a particular tool.

%Also important in further developing corpus methods is the use of \emph{distributed version control} of data and code via tools such as \emph{Git}. Distributed sharing of code makes it possible to use and improve others' code, radically enhancing reproducibility, transparency, and the ability to communicate results. The Git repositories produced alongside this thesis make it possible for others to re-perform investigations, modify parameters, or use methods on new corpora. 

%\subsubsection{Methodological advances}

%The case study presented here demonstrates that new things can be learned, with a greater level of accuracy and automation, by utilising new tools, data and methods when doing CL\slash CADS research. Key contributions are outlined below.

\subsubsection{Indigenous reference corpora}

In mainstream \gls{CL}, word frequency counts in reference \glspl{corpus} are commonly used to identify keywords---that is, words that are unusually frequent in target corpus compared to the reference corpus according to some statistical measure (see Section \ref{sect:cl}). The aim of keywording, generally, is to locate words that are somehow salient within the corpus (e.g. words that index common topics of conversation, or the interactants within \gls{corpus} texts). As discussed in Chapter \ref{chap:approaches}, the use of `balanced' reference \glspl{corpus} as a means of generating keywords is inherently problematic from a theoretical perspective. When using such `balanced' \glspl{corpus}, it is not possible to determine the extent to which contents of the \gls{corpus} are representative of language use as a whole, nor is it possible to know what constitutes an appropriate weighting of different text types contained within. More accurately, there is no such thing as a `balanced', `general' corpus in the first place. When a person does \gls{CL}, what is being investigated is the frequency of linguistic phenomena in a particular collection of instances of language use, or texts. What this is being compared to---the reference corpus---is not a representation of the probabilities inherent to the system of language, but simply a second (often larger) collection of instances. No matter the size and contents of the reference \gls{corpus}, it can never contain the general probabilities for features in a language, because these probabilities are not encoded in the system until language has interacted with a situational context (i.e. a register) during the process of instantiation.

An associated problem with the use of reference \glspl{corpus} is that the registers they contain are often inappropriate for answering particular research questions. In the case of the \glslink{Forum}{Bipolar Forum}, it makes little sense to compare the corpus to an arbitrary collection of British English texts (including short stories, speeches, etc.), unless the research question concerns the difference between language in the \gls{Forum} and language in the BNC. More useful in almost every case would be to compare language in the \gls{Forum} to that of other \glspl{forum}, or to language use in an offline \gls{bipolar} community. In systemic terms, most often, the ideal reference \gls{corpus} should often differ in a single register variable (Field, Tenor or Mode) from that of the target \gls{corpus}. For these reasons, the case study did not involve the use of a reference \gls{corpus} of general English text. Instead, two alternative strategies were presented for extraction of salient \glslink{lexicogrammar}{lexicogrammatical} features from the \glslink{Forum}{Bipolar Forum}. First, keywording was performed on subsections of the \gls{corpus}, using the entirety of the \gls{corpus} as the reference material. This made it possible to locate salient words at different stages of membership, at different points in time, and in the language of members who would go on to become veteran contributors, when compared to those who dropped out soon after joining. The second strategy is to use parsing to isolate words based on grammatical position, and to calculate the relative frequency of each. This method proved surprisingly accurate: simply counting the relative frequencies of participant heads was able to closely approximate a keyword list. This approach has the added usefulness of being able to quickly distinguish between grammatical roles, so that salient participants and processes could be considered separately, and so that experiential components could be considered apart from, for example, \sctext{Mood} and \sctext{Modality} choices. This approach appears to be superior to keywording performed without restriction to grammatical role: the fact that participants outnumber processes in most registers of English, for example, means that participants will tend to dominate frequency lists, despite the centrality of processes to experiential meaning. This problem is exacerbated when lemmatisation is not performed, as verb inflections will be counted individually.

The proposed approach has some notable drawbacks. The most obvious is that texts must first be parsed, which can be a computationally intensive process, and which can be unsuitable for certain text types, for which parser models are not available. Manipulating parsed datasets is also more complicated than plain text files (though the developed software works toward the aim of increasing the ease with which parsed data can be queried). Finally, as mentioned earlier, parsing has occasionally been construed by \gls{CL} practitioners as a (problematic) theoretical imposition on texts. In these cases, however, preference should be given to a theoretical imposition that is well\hyp{}constructed and empirically informed, over reliance on arbitrary stopword lists and general \glspl{corpus} that even by proponents' accounts \cite[e.g.]{baker_acceptable_2012} are only impossible theoretical ideals.

\subsubsection{Improving normalised frequency calculation}

%todo: get evert 2015 :)

Corpus approaches to register \cite[including much of Biber's work, e.g.][]{biber_quantitative_2001,biber_register_2012} have aimed to create quantitative sketches of a \gls{corpus} or its subcorpora by counting relative frequencies of \glslink{lexicogrammar}{lexicogrammatical} features (passives, interrogatives, \emph{wh}\hyp{}pronouns, etc.). These relative frequencies in these studies are typically calculated by dividing the target feature by some constant, such as \emph{per million words}. As others (e.g. Evert, 2015) have pointed out, however, the use of a constant denominator is often inappropriate. As shown in Chapter \ref{chap:experiential}, \emph{support} emerges as a key process in veteran discourse, as veterans explain to newcomers that the community is a site for exchange of both health information and interpersonal care. If we search for \emph{support} as a process in the Membership Stage Structure, we will find a decreasing frequency per million words, however. The reason for this is that new users write more formally, with a slightly higher lexical density (see Figure \ref{fig:derived_shallow_P}). As such, clauses contain fewer processes, and more participants. Thus, we can easily misinterpret the quantitative results to understand \emph{support} as less and less common, when in reality, it is a key difference in the processes selected at early and late stages of membership. The change in lexical density can in this way also be easily overlooked. A key advantage of the approach taken in the case study is that normalised frequency generation can be trivially and consistently applied: modalised clauses are divided by the number of clauses; \emph{would\slash will} are divided by the number of modals, and so forth.

% test it with corpkit
% from corpkit import *
% corp = corpus('bp')
% processes = corp.interrogate(T, r'/VB.?/ >># ( VP >+(VP) (VP !> VP $ NP))',
%                              show=L, multiprocess = 2)
% words = corp.interrogate(T, r'/[A-Za-z0-9]/ !< __', show = C)
%relword = processes.edit('/', words.results, sort_by = 'increase', keep_stats = True)
%relproc = processes.edit('/', SELF, sort_by = 'increase', keep_stats = True)
%for res in list(relproc.results)[:500]:
%    proc_slope = relproc.results[res]['slope']
%    word_slope = relword.results[res]['slope']
%    if proc_slope > 0 and word_slope < 0:
%        print 'Positive slope for process, negative slope for word: %s' % res
%    elif proc_slope < 0 and word_slope > 0:
%        print 'Negative slope for process, positive slope for word: %s' % res

\subsubsection{Collapsing the corpus\slash computational distinction}

For the case study of this thesis, corpus linguistic tasks such as interrogation and keywording were performed with the aid of computational linguistic tools---namely, the \texttt{Stanford CoreNLP} suite. In light of the affordances of this approach, it is useful to stress at this point that corpus and computational linguistics are in many senses isomorphic, existing on a continuum of `digital linguistics'. The key difference is that \gls{CL} tends to orient toward interpretation of datasets as a goal, while computational linguistics is generally concerned with the development of automated tools and workflows for processing (arbitrary) texts. As such, programming is central to computational linguistics, but peripheral within \gls{CL}; conversely, solid theoretical linguistic grounding is generally a requirement within \gls{CL}, but increasingly seen as unnecessary in computational linguistics (with the shift from rule\hyp{}based to probabilistic, machine learning approaches to grammar).
%todo, above: reference jurafsky?

Blurring the line between the two fields, as was done for the case study presented here, has distinct advantages. First, it allows corpus linguists to have a greater say in the kinds of language processing tools being built. Though tools developed in \gls{NLP} may be explicitly aimed at non\hyp{}computational researchers \cite{de2008stanford}, the current distinction between the two disciplines means that \gls{CL} practitioners have no channel through which their needs can be articulated. At the same time, computational linguists are afforded a greater access to relevant theory and downstream applications. One example of this symbiotic relationship is in computational recent work on topic modelling---a means of classifying the `topic' of documents through analysing the co\hyp{}occurrence of lexical items \cite{blei2003latent}. Despite widespread use elsewhere \cite[e.g. history\slash digital humanities---see][]{blevins_topic_2010,brauer_digital_2014,yang_topic_2011}, topic modelling has had poor take\hyp{}up within \gls{CL}. A key reason for this is that it willfully ignores key theoretical assumptions about how language makes meaning \cite{boyd-graber_syntactic_2009}. Under this approach, each lexical item in a text is given equal weight. An arbitrary list of high\hyp{}frequency stopwords is generally used during pre\hyp{}processing to remove tokens that are assumed to carry little topic meaning. Such approaches, do not engage with the fact that the grammatical position of words plays an important role in determining their centrality to experiential meanings \cite{halliday_introduction_2004}. Collaboration on this front has led to topic modelling algorithms that outperform theory\hyp{}free approaches, while satisfying theoretical demands for data\hyp{}driven design (Rubino \& McDonald, forthcoming).

%As such, insight from theory-driven \gls{CL} can improve performance in key computational tasks.

%Topic modelling is based on the `bag-of-words' assumption, also known as the exchangeability assumption, for words in a document~\cite{aldous1985exchangeability}, has been shown to be a powerful approach of dimensionality reduction by finding an underlying hidden thematic structure within a document collection. 

\subsection{Corpus assisted discourse studies}

%todo wording 'mostly'
\gls{CL} methods are used for a diverse range of research interests, ranging from lexicography to communities of practice. In being centred on investigating the relationship between \gls{lexicogrammar} and \glspl{discourse-semantic}, the case study here is related mostly to \gls{CADS}. Below, implications are outlined for this body of literature.

\subsubsection{Exploiting parser output for discourse features}

The most central contribution of the case study to \gls{CADS} is in demonstrating the utility of extracting discursively significant phenomena from \glslink{lexicogrammar}{lexicogrammatical} parses. Key to the process is in shifting between constituency and dependency parses: constituency grammars emphasise group\slash phrase structure and linear ordering of tokens, while dependency grammar emphasises argument types and functional roles. As such, it seems that \sctext{Mood} (and presumably \sctext{Theme}) phenomena are best searched for using constituency parses, while \sctext{Transitivity} phenomena relate more closely to dependency parser output \cite{costetchi_method_2013}. The approach developed for automatically linking \gls{lexicogrammar} to discourse is so far, a modest one, centred on the use of wordlists that collapse processes into types, and participants along taxonomic lines. This allowed frequency counting of different kinds of participants (\emph{The Self}, \emph{Other Members}, \emph{Health Professionals}, and \emph{Friends\slash Family}). From these categories, combined with systemic functional Process Type wordlists, it was possible to show how various participant taxonomies \cite[see][]{martin_english_1992} rise to prominence at later stages of membership, and at different points in the \gls{Forum}'s history.

Despite the successes of the case study in translating \glslink{lexicogrammar}{lexicogrammatical} parses into patterns of meaning, it is important to acknowledge that such approaches cannot yet replace what is achieved in qualitatively oriented discourse analysis. Insights gained from sustained, manual analysis of individual texts\hyp{}in\hyp{}context are exceptionally difficult to approach using the methods presented here. The key reason for this is that automated processing of grammatical metaphor, as well as automated analysis of \glslink{lexicogrammar}{lexicogrammatical} choices across the clause\hyp{}complex, is a field still very much in its infancy. Improvement in these tasks will likely involve significant collaboration between computational linguists and expert human coders. Moreover, as proposed in Chapter \ref{chap:onlinehealth}, manual annotation of semantic phenomena could, in an interdisciplinary project, be used to train an automatic classifier that could annotate unseen text.

\subsubsection{Automating thematic analysis}

As discussed in Chapter \ref{chap:approaches}, \gls{CADS} typically relies on counting of lexis, and on concordancing of lexis to determine context of use. Missing in the current generation of \gls{corpus} tools, as well as the \gls{CADS} literature, however, is recognition that \gls{corpus} interrogation for feature frequency counting and full concordancing are in fact more or less the same task. Techniques such as n\hyp{}gram counting, where researchers locate a list of \emph{n} lexical items that appear adjacently, occupy the middle\hyp{}ground between the two poles (see Table \ref{tab:CADSmethods}). Concordancing simply uses human judgement of co\hyp{}text to determine whether a search result conforms to or does not conform to some pattern in the content stratum. When researchers thematically code concordance lines, they are inherently responding to \glslink{lexicogrammar}{lexicogrammatical} choices made in the clause(s) surrounding a lexical item of interest. Therefore, as \gls{corpus} query languages and annotation conventions evolve, and as more abstract grammatical features can be traversed automatically, it becomes feasible to replace manual with automatic methods of categorisation. In many current use\hyp{}cases for concordancing, the pattern matching could already potentially be automated. If we want to see how The Self is appraised, for example, we can search for Values when the Token is \emph{I} (or, less commonly, adjectival modifiers of \emph{me} as Thing).

\begin{table}[htb]
    \begin{tabularx}{\textwidth}{llll}
    \toprule 
    \textbf{Method}   & \textbf{Stratum} & \textbf{Rank} & \textbf{Further analysis}  \\
    \midrule
    \textbf{Close reading}  & Lexicogrammar & Text\slash Clause-complex     & Manual      \\ 
    \textbf{Concordancing}  & Lexicogrammar & Clause-complex\slash clause   & Manual         \\  
    \textbf{Parse interrogation}  & Lexicogrammar & Clause-complex\slash clause   & Automatic    \\
    \textbf{N-gramming}  & Lexicogrammar & Phrase\slash group  & Automatic    \\
    \textbf{Keywording}  & Lexis         & Word                          & Automatic \\   
    \bottomrule
    \end{tabularx}
    \caption[Methods in CADS and the cline of instantiation]{Methods in CADS and their coverage of the cline of instantiation, ordered by rank targeted}
    \label{tab:CADSmethods}
\end{table}

%todo: false positives can be removed from concordances, and frequencies recalculated automatically.
Using \texttt{corpkit}, concordances are produced by default during every query. The concordance lines could then be used to determine that queries were capturing intended phenomena, to remove false positives, and to display typical co\hyp{}text for \glslink{lexicogrammar}{lexicogrammatical} phenomena of interest. This ensures that important meanings are not missed during the reduction of texts to frequency counts. Part of the contribution of the developed tools and methods, therefore, is a scalable approach to thematic categorisation of language at group, phrase, clause and clause\hyp{}complex levels.

\section{Systemic-functional linguistic theory}

\gls{SFL} was used as the dominant underlying theory of language for the analysis of language use in the \gls{Forum}. As explained in Chapter \ref{sect:sfl}, a major motivation for this was \gls{SFL}'s metafunctional division, which facilitated separate analyses of interpersonal and experiential meanings in the \glslink{Forum}{community}. Also important is that \gls{SFL} provides a well\hyp{}articulated connection of morphosyntax to meaning\hyp{}making and the performance of social action. Alongside other recent work \cite[e.g.][]{coffin_using_2013,hunston_systemic_2013}, the case study highlights the usefulness of \gls{SFL} in \gls{CL} and \gls{CMC} research. Contributions for \gls{SFL} are twofold: first, I highlight some theoretical issues uncovered during the course of the investigation. Second, I discuss practical issues in the use of \gls{SFL} alongside currently available linguistic technology for \gls{CL} research interests.

\subsection{Theoretical contributions}

The case study used only a fairly superficial rendition of the \gls{SFG}, constrained by what could be extracted from constituency and dependency parses of texts. As such, theoretical contributions are best understood as discussion points and unresolved questions, rather than suggested changes to systemic theory.

\subsubsection{Ergative transitivity and corpus methods}

\noindent Process Types are a notoriously difficult part of the \gls{SFG} to automatically label and\slash or extract \cite{costetchi_method_2013}. Even within the \gls{SFL} community, multiple interpretations of the Process Type system are in use \cite[c.f. the \emph{Cardiff Grammar}---][]{fawcett_theory_2000}. The reason for this difficulty is that lexical choices for main verbs are often metaphorical: \emph{run}, for example, can variously realise a behavioural (\emph{to run a mile}), material (\emph{to run for office}), or even verbal process (\emph{to run one's mouth}). An added problem is that many verbs exhibit a high degree of indeterminacy, necessitating grammatically and\slash or semantically informed disambiguation \cite{odonnell_survey_2009,gwilliams_indeterminacy_2015}. In the best case scenario, Process Types in ambiguous cases can be resolved by looking at their transitivity structure: as an intransitive, \emph{appear} is generally material; when transitive, it is more likely relational; many Verbiages are tensed, embedded clauses, which can help in disambiguating verbal processes \cite{gwilliams_indeterminacy_2015}. In many other cases, however, ambiguities are far more difficult to resolve.

The case study demonstrated the usefulness of the ergative model of \sctext{Transitivity} outlined by \textcite{halliday_notes_1968,halliday_notes_1967-1,halliday_notes_1967}. The ergative participant function of \emph{Agent} is particularly useful in understanding which participants are construed as having agency---that is, being able to do things and make things happen (in material processes, according to the transitive model), to think things (in mental processes), say things (in verbal processes), and to attribute and assign values (in relational processes). The \glslink{Forum}{Bipolar Forum} \glslink{member}{users} shift from a pattern of Self\hyp{}as\hyp{}Medium toward Self\hyp{}as\hyp{}Agent over the course of membership, construing themselves as collaborators in health decision\hyp{}making processes, and, at the same time, as responsible for managing manic and depressive episodes in the \gls{bipolar} cycle. Meanwhile, the agency of health professionals decreases over time: health professionals perform diagnosis and prescribe treatments to the Self\hyp{}as\hyp{}Medium in early \glspl{post}, but, over time, become Mediums themselves, through which events such as medication management are carried out. As \textcite{halliday_introduction_2004} explain, the transitive and ergative models of \sctext{Transitivity} are complementary in language, and thus in its analysis. In this investigation, the increased level of generality in the ergative grammar was found to be suitable for uncovering more general patterns in \sctext{Transitivity} as a representation of social actors; the transitive model of \sctext{Transitivity} is better employed when the researcher prefers to conceptualise unfolding experiential semantics as sequences of quanta of change \cite{halliday1999construing}.

\subsubsection{Interaction between the metafunctions}

While much has been written about intra\hyp{}stratal relationships, both theoretically \cite[e.g.][]{hasan_structure_1985}, and empirically \cite[e.g.][]{clarke_patterns_2012}, less attention has been paid to the relationship between the metafunctions within \gls{SFL} and the \gls{SFG}. As metafunctions are realised through predominantly overlapping components of lexicogrammar, it is obvious that they cannot be completely independent variables: choices made for one metafunction can affect, on some level, the possible realisations in another. It is plausible to suggest, therefore, an \textbf{interplay between the metafunctions}, where changes in one dimension result in changes within another. Put differently, the differences between the most experiential meanings made in the \glslink{Forum}{Bipolar Forum}, and in emergency room interactions, is best understood in relation to what distinguishes the situations in the dimensions of \gls{Mode} and Tenor. In terms of \gls{Mode}, reflective and time\hyp{}unlimited turns open up space for construal of Events and Things only loosely related to the overarching purpose of the community, or things that may be of little importance or relevance to other \gls{Forum} \glslink{member}{users}. Likewise, in terms of the Tenor of discourse, the relatively equal consumer--consumer power dynamic opens up room for a broader range of appropriate topics than is generally possible within a hospital or a clinic, where the power\hyp{}unequal nature of interactions between professionals and consumers in hospitals and clinics, as well as the time\hyp{}critical \gls{Mode}, can both also manifest experientially in a narrowed semantic field.

Another Tenor\slash \gls{Mode}\hyp{}driven cause of experiential meaning choices is the apparent absence of health professionals in the community. In not being co\hyp{}present, but in being commonly encountered by most community members, health professionals can be freely discussed, with a reduced risk of loss of face for interactants. This allows users to question the appropriateness of their and others' diagnoses or treatment plan. As shown in Chapter \ref{chap:experiential}, veteran \glslink{member}{users} may go so far as to construe health professionals as ignorant of the needs of those living with \gls{bipolar}, and of the phenomenology of the illness itself. Such challenges to the institutionally prescribed power of health professionals over health consumers are unlikely to take place within formal healthcare institutions.

%The metafunction hookup hypothesis conceptualises an interrelatedness of Field, Tenor and Mode variables, whereby changes in one dimension are likely to have ramifications in another. Broadly, 

\subsection{Practical contributions}

The second area in which the thesis contributes to \gls{SFL} is in documenting the extent to which non\hyp{}systemic grammars can be used for systemic analysis.

%todo: add tools for registerial cartography

\subsubsection{Accounting for genre}

\noindent One difficulty arising in the investigation was accounting for the influence of genre: first contributions contained a high frequency of \emph{I am bipolar} constructions, with a number conforming to the well\hyp{}known formulaic self\hyp{}introduction of Alcoholics Anonymous meetings (see Chapter \ref{chap:experiential}). Though first \glspl{post} were treated as a stage of membership (and though, of course, the first \gls{post} does represent a stage of membership), first \glspl{post} are subject to particularly strong generic expectations: many users provide an account that stretches back to childhood, or to the onset of symptoms of bipolar or a similar condition; \glslink{member}{users} typically include the events and circumstances surrounding their diagnosis, and provide a coda appraising their current situation or past choices. These generic expectations lead to a unique quantitative profile for first \glspl{post} when compared to \glspl{post} made at all other stages of membership. This effect poses a difficulty for automated analysis: in this case study, the generic expectations provide an unwanted variable when attempting to chart longitudinal change.

%The computational workflow described below can automatically identify texts that differ in one or more linguistic dimensions from other texts in the corpus. % Evaluating the \emph{cause} of such

\subsubsection{Ergative approaches}

\noindent Developments in systemic parsing are ongoing. Existing methods, however, tend to be based on post\hyp{}processing of \texttt{CoreNLP} parser output. Moreover, annotation of the \gls{discourse-semantic} stratum is still a far\hyp{}off goal. A central issue in systemic parsing is that participant role labels are selected by the Process Type. Incorrect Process Type identification therefore leads to incorrect participant labelling. One potential solution is to rely on the ergative transitivity model. Ergative modelling of transitivity requires a smaller label set, with many components determinable based on the existence or non\hyp{}existence of an object argument of the main verb.

\subsubsection{Quantitative register modelling}

%TODO: check
The quantitative modelling of registers is potentially useful for both theoretical and computational linguistics. Theoretically, quantitative register models could be used to build taxonomies and topologies of language as a network of registers \cite{matthiessen_modeling_2015}. Computationally, register models could improve lexicogrammatical parsing and parse re\hyp{}ranking, or facilitate dedicated parsing of the semantic stratum. 

While \gls{SFL} provides a delineation of register into Field, Tenor and \gls{Mode}, and identifies the linguistic systems that relate to each, there does not yet appear to be a consensus regarding which linguistic features should be contained within a computational register model, or how the model should be built, stored or shared. Regarding possible features to include, the case study revealed that shallow features only tell part of the registerial story of a \gls{corpus}: the syntagmatic behaviour of frequent or key lexical or lexicogrammatical features can also index discourses and ideologies within texts. The tools developed for this study certainly present one potential approach to a standardised modelling of register as one or more two-dimensional arrays. A set of interrogations could be defined as a script or routine, run over novel \glspl{corpus}, and added to a common (open-access, version controlled) model. The model could then be incorporated into a number of computational linguistic tasks: arbitrary texts could then be classified based on their similarity to modelled registers; a given \gls{corpus} could be compared and contrasted with others, in order to locate suitable reference \glspl{corpus}, and so forth.

\section{Health discourse}

Language performs a number of roles in healthcare: it may be used to describe or elicit descriptions of symptoms; it may facilitate procedures; it may be consulted for information in reference books \cite{matthiessen_applying_2013}. This centrality of language to the institution of healthcare means that language use can be related to the success of treatment \cite{divi_language_2007}. As such, linguistic analysis of \gls{HC} can lead to interventions that lower the risk of avoidable patient harm \cite{slade_communicating_2015}.

Most existing work on \gls{HC} has centred on hospitals and clinics as settings, on intra\hyp{}professional or professional--consumer interactions as Tenor configurations, and on face\hyp{}to\hyp{}face, spoken language as \gls{Mode}. These contexts are undoubtedly of immense importance: miscommunication poses a serious risk to patients, especially in high-stakes emergency room or multilingual contexts \cite{slade_role_2015,slade_effective_2015}. Such data, however, is expensive and time-consuming to produce, requiring large-scale collaboration between universities and healthcare settings, and the accompanying ethical protocols; it is often unpredictable in terms of quality, content and structure.

The case study presented here, however, demonstrates that very large datasets can be efficiently built using existing sites of \gls{CMC}. OSG literature reviewed in Chapter 2 has also shown the feasibility of creating health-focussed online communities with the explicit aim of generating data for applied linguistic analysis \cite[e.g.][]{johnson_emergence_2015}. This comes at the expense of access to the face\hyp{}to\hyp{}face \gls{Mode} variable, and often, the potential for follow-up interviews. Ethics may be more difficult to define and implement, as national guides may not provide clear and comprehensive guidelines for working with \gls{CMC}. The affordances of digital data are many, however. First, it is relatively trivial to collect enough information to form quantitatively reliable accounts of language use. Second, data is amenable to both automatic collection and automatic analysis. Third, the method is scalable---a well designed workflow can potentially be applied to a new online community by doing little more than selecting a different URL to harvest.

Different Tenor variables provide a further key affordance: not only can we understand the ways in which relationships between interactants are formed, but also, we are given access to colourful construals of the non-present participants in health discourse. Because of the non-presence of health professionals in the \glslink{Forum}{Bipolar Forum}, consumers are free to construe them in ways which could be potentially face\hyp{}threatening within clinical encounters. Just as analysis of communication between nurses has provided insights into the ways doctors and patients are construed, \glspl{OSG} provide representations of healthcare professionals that cannot be obtained through analysis of clinical encounters. Similarly, opinions that may diverge from normative biomedical discourse can also be aired.

\section{Addressing Question 3: Needed tools and methods}

Tool development comprised a substantial part of the overall research design. The primary reason for this, as noted in Chapter \ref{chap:researchdesign}, is that existing tools were insufficient for some of the techniques presented in the case study analysis. Generally speaking, tools with graphical interfaces pose hard constraints on what kinds of things can be searched for and calculated. Command line tools, on the other hand, while being more flexible, are often centred on a single task (searching a single text; performing a calculation; drawing a figure) rather than larger workflows that require quickly shifting between these tasks. Though a combination of multiple tools could replicate some of the findings presented here, constant switching between tools has serious drawbacks. First, such a method is difficult to repeat or reproduce. Second, a lack of uniformity in the ways tools work with text can lead to inaccuracies: tokenisation and lemmatisation are handled differently by different tools, and in many cases, cannot be customised by the user. Third, some tools are proprietary, and may obscure the algorithms that underlie result generation. Fourth, the time and effort involved in moving between interfaces, importing and exporting data, and keeping records of what has been done would be substantial. Finally, adopting such a method would not save future researchers time or effort.

As a result of these considerations, purpose-built tools were constructed for the case study analysis. At the same time, however, the tools were designed for more general use within any of the intersecting disciplines interested in analysis of digital text (digital humanities, \gls{CL}, \gls{NLP}, etc.). That is to say, the computational infrastructure was not built around the dataset, but around a more general conceptualisation of linguistic datasets as structured collections of digitised text. The aim, therefore, was to develop software that could both facilitate analysis of the Bipolar Forum and which could allow researchers to undertake similar kinds of analysis on arbitrary datasets in the future.

% The developed methodology relies on a number of discrete computational processes, linguistic and non\hyp{}linguistic. Linguistic processes incluce tokenisation, parsing, lemmatisation, inflection generation, querying, spelling correction and the like. Non\hyp{}linguistic processes include statistical analyses, visualisations, data management, and so on. Though tools exist for each of these tasks, no existing software brought all together into a single module. For this reason, it was necessary to construct tools. The development of these tools means that they are inherently suitable for the tasks at hand. At the same time, however, the novelty of the methods means that it is difficult to assess the accuracy of each lexicogrammatical query. Novel methods more generally make it more difficult to link findings to those wihin related research.  Furthermore, time spent developing tools limits time spent subtracts from time spent analysing data. Overall, however, tool development ultimately saved time, and was the only means of ensuring that results are accurate and reproducible.


\section{Implications: a summary}

The findings of the case study, as well as the methods used to generate them, have implications for \gls{CL}, \gls{SFL} and \emph{HC}. At the level of theory, new parts of the healthcare journey have been sketched from both quantitative and qualitative linguistic perspectives. At the same time, the production of an open\hyp{}source tool for creation and analysis of parsed and structured \glspl{corpus} expands the kinds of research questions that can be answered using corpus methods. The next chapter provides a short research agenda, a summary of the thesis, and a conclusion centred on possible integration of the tools and methods developed in this case study within outcome\hyp{}driven \gls{HC} research.


